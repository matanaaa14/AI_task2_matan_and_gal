{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRVHuHonarhJsUZdjwY08z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matanaaa14/AI_task2_matan_and_gal/blob/main/bellman_f.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9IJteaK2Kdj",
        "outputId": "336f87c1-b93b-4572-9045-e1c28f98a3a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running Test Case 1\n",
            "Policy Score (Value Iteration): 0.8222000000000033\n",
            "Value Iteration Time: 0.0041 seconds\n",
            "\n",
            "Policy:\n",
            "→ → → P\n",
            "↑ W ↑ N\n",
            "↑ → ↑ ←\n",
            "\n",
            "Utilities:\n",
            "╒═══════╤═══════╤══════╤═══════╕\n",
            "│ 0.08  │ 0.27  │ 0.74 │   1   │\n",
            "├───────┼───────┼──────┼───────┤\n",
            "│ -0.01 │   W   │ 0.24 │  -1   │\n",
            "├───────┼───────┼──────┼───────┤\n",
            "│ -0.05 │ -0.02 │ 0.06 │ -0.02 │\n",
            "╘═══════╧═══════╧══════╧═══════╛\n",
            "\n",
            "Running Test Case 2\n",
            "Policy Score (Value Iteration): 93.21500000000002\n",
            "Value Iteration Time: 0.0050 seconds\n",
            "\n",
            "Policy:\n",
            "→ → → P\n",
            "↑ W ↑ N\n",
            "↑ → ↑ ←\n",
            "\n",
            "Utilities:\n",
            "╒══════╤══════╤══════╤══════╕\n",
            "│ 0.24 │ 0.43 │ 0.9  │  1   │\n",
            "├──────┼──────┼──────┼──────┤\n",
            "│ 0.15 │  W   │ 0.4  │  -1  │\n",
            "├──────┼──────┼──────┼──────┤\n",
            "│ 0.11 │ 0.14 │ 0.22 │ 0.14 │\n",
            "╘══════╧══════╧══════╧══════╛\n",
            "\n",
            "Running Test Case 3\n",
            "Policy Score (Value Iteration): -3.433\n",
            "Value Iteration Time: 0.0109 seconds\n",
            "\n",
            "Policy:\n",
            "→ → → P\n",
            "↑ W ↑ N\n",
            "↑ → ↑ ←\n",
            "\n",
            "Utilities:\n",
            "╒═══════╤═══════╤═══════╤═══════╕\n",
            "│ -1.84 │ -1.65 │ -1.18 │   1   │\n",
            "├───────┼───────┼───────┼───────┤\n",
            "│ -1.93 │   W   │ -1.68 │  -1   │\n",
            "├───────┼───────┼───────┼───────┤\n",
            "│ -1.97 │ -1.94 │ -1.86 │ -1.94 │\n",
            "╘═══════╧═══════╧═══════╧═══════╛\n",
            "\n",
            "Running Test Case 4\n",
            "Policy Score (Value Iteration): -16.018\n",
            "Value Iteration Time: 0.0210 seconds\n",
            "\n",
            "Policy:\n",
            "→ → → → → → → → → → → ↓\n",
            "→ → → → → → → → → → → ↓\n",
            "→ → → → → → → → → → → ↓\n",
            "↑ N N N N N N N N N N P\n",
            "\n",
            "Utilities:\n",
            "╒════╤══════╤══════╤══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╕\n",
            "│ -2 │  -2  │  -2  │  -2  │  -2   │  -2   │ -1.99 │ -1.98 │ -1.97 │ -1.94 │ -1.88 │ -1.75 │\n",
            "├────┼──────┼──────┼──────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│ -2 │  -2  │  -2  │  -2  │  -2   │ -1.99 │ -1.98 │ -1.97 │ -1.94 │ -1.88 │ -1.75 │ -1.5  │\n",
            "├────┼──────┼──────┼──────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│ -2 │  -2  │  -2  │  -2  │ -1.99 │ -1.98 │ -1.97 │ -1.94 │ -1.88 │ -1.75 │ -1.5  │  -1   │\n",
            "├────┼──────┼──────┼──────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│ -2 │ -100 │ -100 │ -100 │ -100  │ -100  │ -100  │ -100  │ -100  │ -100  │ -100  │   1   │\n",
            "╘════╧══════╧══════╧══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╛\n",
            "\n",
            "Running Test Case 5\n",
            "Policy Score (Value Iteration): -18.06\n",
            "Value Iteration Time: 0.1081 seconds\n",
            "\n",
            "Policy:\n",
            "→ → → → → → → → → → → ↓\n",
            "↓ → → → → → → → → → → ↓\n",
            "↓ ↓ → → → → → → → → → ↓\n",
            "↓ ↓ → → → → → → → → → ↓\n",
            "→ → → → → → → → → → → ↓\n",
            "↓ N N N N N N N N N N P\n",
            "\n",
            "Utilities:\n",
            "╒════╤══════╤══════╤══════╤══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╕\n",
            "│ -2 │  -2  │  -2  │  -2  │  -2  │  -2   │  -2   │  -2   │  -2   │ -1.99 │ -1.99 │ -1.97 │\n",
            "├────┼──────┼──────┼──────┼──────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│ -2 │  -2  │  -2  │  -2  │  -2  │  -2   │  -2   │  -2   │ -1.99 │ -1.99 │ -1.97 │ -1.94 │\n",
            "├────┼──────┼──────┼──────┼──────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│ -2 │  -2  │  -2  │  -2  │  -2  │  -2   │  -2   │ -1.99 │ -1.99 │ -1.97 │ -1.94 │ -1.87 │\n",
            "├────┼──────┼──────┼──────┼──────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│ -2 │  -2  │  -2  │  -2  │  -2  │  -2   │ -1.99 │ -1.99 │ -1.97 │ -1.94 │ -1.87 │ -1.71 │\n",
            "├────┼──────┼──────┼──────┼──────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│ -2 │  -2  │  -2  │  -2  │  -2  │ -1.99 │ -1.99 │ -1.97 │ -1.94 │ -1.87 │ -1.71 │ -1.38 │\n",
            "├────┼──────┼──────┼──────┼──────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│ -2 │ -100 │ -100 │ -100 │ -100 │ -100  │ -100  │ -100  │ -100  │ -100  │ -100  │   1   │\n",
            "╘════╧══════╧══════╧══════╧══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╛\n",
            "\n",
            "Running Test Case 6\n",
            "Policy Score (Value Iteration): -10.5\n",
            "Value Iteration Time: 0.0208 seconds\n",
            "\n",
            "Policy:\n",
            "N ↑ → ↓ ←\n",
            "← → → P ←\n",
            "↓ ↓ ↑ ↑ ↑\n",
            "→ P ← ↑ →\n",
            "→ ↑ ← ↓ N\n",
            "\n",
            "Utilities:\n",
            "╒═══════╤═══════╤═══════╤═══════╤═══════╕\n",
            "│  -10  │ -0.95 │ -0.44 │ 0.22  │ -0.41 │\n",
            "├───────┼───────┼───────┼───────┼───────┤\n",
            "│ -0.96 │ -0.46 │  0.2  │   2   │ 0.22  │\n",
            "├───────┼───────┼───────┼───────┼───────┤\n",
            "│ -0.72 │ -0.39 │ -0.41 │  0.2  │ -0.44 │\n",
            "├───────┼───────┼───────┼───────┼───────┤\n",
            "│ -0.39 │   1   │ -0.39 │ -0.46 │ -0.95 │\n",
            "├───────┼───────┼───────┼───────┼───────┤\n",
            "│ -0.7  │ -0.39 │ -0.72 │ -0.96 │  -10  │\n",
            "╘═══════╧═══════╧═══════╧═══════╧═══════╛\n",
            "\n",
            "Running Test Case 7\n",
            "Policy Score (Value Iteration): 0.58575\n",
            "Value Iteration Time: 0.0103 seconds\n",
            "\n",
            "Policy:\n",
            "→ → → ↓ N\n",
            "→ → → P ←\n",
            "↓ ↓ N ↑ ↑\n",
            "→ P ← ↑ ↑\n",
            "↑ ↑ ← ↑ ↑\n",
            "\n",
            "Utilities:\n",
            "╒═══════╤═══════╤═══════╤═══════╤═══════╕\n",
            "│ -0.37 │ -0.23 │ 0.07  │ 0.72  │  -1   │\n",
            "├───────┼───────┼───────┼───────┼───────┤\n",
            "│ -0.24 │ 0.05  │  0.7  │   2   │ 0.72  │\n",
            "├───────┼───────┼───────┼───────┼───────┤\n",
            "│ -0.21 │ 0.11  │  -2   │  0.7  │ 0.07  │\n",
            "├───────┼───────┼───────┼───────┼───────┤\n",
            "│ 0.11  │   1   │ 0.11  │ 0.05  │ -0.23 │\n",
            "├───────┼───────┼───────┼───────┼───────┤\n",
            "│ -0.2  │ 0.11  │ -0.21 │ -0.24 │ -0.37 │\n",
            "╘═══════╧═══════╧═══════╧═══════╧═══════╛\n",
            "\n",
            "Running Test Case 8\n",
            "Policy Score (Value Iteration): -0.1155\n",
            "Value Iteration Time: 0.0295 seconds\n",
            "\n",
            "Policy:\n",
            "→ → → → → ↓ ↓\n",
            "↓ N ↑ → → P ←\n",
            "↓ ← → → → ↑ ↑\n",
            "→ → → → ↑ ↑ ↑\n",
            "→ → → → → ↓ ↓\n",
            "↓ N ↑ → → P ←\n",
            "→ ← → → ↑ ↑ ←\n",
            "\n",
            "Utilities:\n",
            "╒═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╕\n",
            "│ -0.97 │ -0.92 │ -0.82 │ -0.59 │ -0.05 │ 1.19  │ 0.04  │\n",
            "├───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│  -1   │  -6   │ -0.88 │ -0.12 │ 1.13  │   4   │ 1.19  │\n",
            "├───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│ -0.99 │ -0.99 │ -0.81 │ -0.55 │ -0.02 │ 1.13  │ -0.05 │\n",
            "├───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│ -0.98 │ -0.96 │ -0.91 │ -0.79 │ -0.55 │ -0.1  │ -0.59 │\n",
            "├───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│ -0.99 │ -0.98 │ -0.95 │ -0.88 │ -0.74 │ -0.43 │ -0.73 │\n",
            "├───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│  -1   │  -4   │ -0.97 │ -0.78 │ -0.46 │   1   │ -0.45 │\n",
            "├───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│  -1   │  -1   │ -0.95 │ -0.88 │ -0.74 │ -0.45 │ -0.75 │\n",
            "╘═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╛\n",
            "\n",
            "Running Test Case 9\n",
            "Policy Score (Value Iteration): -0.054\n",
            "Value Iteration Time: 0.0282 seconds\n",
            "\n",
            "Policy:\n",
            "→ → → → → ↓ ↓\n",
            "↓ N ↑ → → P ←\n",
            "↓ ← → → → ↑ ↑\n",
            "→ → → → ↑ ↑ ↑\n",
            "→ → → → → ↓ ↓\n",
            "↓ N ↑ → → P ←\n",
            "→ ← → → ↑ ↑ ←\n",
            "\n",
            "Utilities:\n",
            "╒═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╕\n",
            "│ -0.97 │ -0.92 │ -0.82 │ -0.59 │ -0.05 │ 1.19  │ 0.04  │\n",
            "├───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│  -1   │  -6   │ -0.88 │ -0.12 │ 1.13  │   4   │ 1.19  │\n",
            "├───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│ -0.99 │ -0.99 │ -0.81 │ -0.55 │ -0.02 │ 1.13  │ -0.05 │\n",
            "├───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│ -0.98 │ -0.96 │ -0.91 │ -0.79 │ -0.55 │ -0.1  │ -0.59 │\n",
            "├───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│ -0.99 │ -0.98 │ -0.95 │ -0.88 │ -0.74 │ -0.43 │ -0.73 │\n",
            "├───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│  -1   │  -4   │ -0.97 │ -0.78 │ -0.46 │   1   │ -0.45 │\n",
            "├───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
            "│  -1   │  -1   │ -0.95 │ -0.88 │ -0.74 │ -0.45 │ -0.75 │\n",
            "╘═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╛\n",
            "\n",
            "Running Test Case 10\n",
            "Policy Score (Value Iteration): 1.8965\n",
            "Value Iteration Time: 0.0234 seconds\n",
            "\n",
            "Policy:\n",
            "→ → → → ↓ ↓ ↓\n",
            "↓ N ↑ W → P ←\n",
            "↓ ← → → ↑ ↑ ↑\n",
            "→ → → → ↑ ↑ ↑\n",
            "→ → → → → ↓ ↓\n",
            "↓ N ↑ W → P ←\n",
            "→ ← → → ↑ ↑ ↑\n",
            "\n",
            "Utilities:\n",
            "╒═══════╤═══════╤═══════╤═══════╤═══════╤══════╤═══════╕\n",
            "│ -0.47 │ -0.42 │ -0.32 │ -0.09 │ 0.45  │ 1.7  │ 0.54  │\n",
            "├───────┼───────┼───────┼───────┼───────┼──────┼───────┤\n",
            "│ -0.5  │  -6   │ -0.42 │   W   │  1.7  │  4   │  1.7  │\n",
            "├───────┼───────┼───────┼───────┼───────┼──────┼───────┤\n",
            "│ -0.49 │ -0.49 │ -0.32 │ -0.07 │ 0.51  │ 1.63 │ 0.45  │\n",
            "├───────┼───────┼───────┼───────┼───────┼──────┼───────┤\n",
            "│ -0.48 │ -0.46 │ -0.4  │ -0.29 │ -0.04 │ 0.4  │ -0.09 │\n",
            "├───────┼───────┼───────┼───────┼───────┼──────┼───────┤\n",
            "│ -0.49 │ -0.48 │ -0.45 │ -0.38 │ -0.24 │ 0.07 │ -0.23 │\n",
            "├───────┼───────┼───────┼───────┼───────┼──────┼───────┤\n",
            "│ -0.5  │  -4   │ -0.48 │   W   │ 0.05  │  1   │ 0.05  │\n",
            "├───────┼───────┼───────┼───────┼───────┼──────┼───────┤\n",
            "│ -0.5  │ -0.5  │ -0.45 │ -0.39 │ -0.24 │ 0.05 │ -0.25 │\n",
            "╘═══════╧═══════╧═══════╧═══════╧═══════╧══════╧═══════╛\n"
          ]
        }
      ],
      "source": [
        "import heapq\n",
        "import numpy as np\n",
        "import random\n",
        "from tabulate import tabulate\n",
        "\n",
        "class GridGameMBRL:\n",
        "    def __init__(self, rows, cols, rewards, success_probability, costs, walls, gamma=0.5):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.rewards = rewards\n",
        "        self.success_probability = success_probability\n",
        "        self.costs = costs\n",
        "        self.walls = walls\n",
        "        self.gamma = gamma\n",
        "        self.value_table = np.zeros((rows, cols))\n",
        "        self.policy = np.zeros((rows, cols), dtype=int)\n",
        "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
        "        self.safety_distance = 2  # Safety distance to negative rewards\n",
        "\n",
        "    def is_valid_location(self, location):\n",
        "        return (0 <= location[0] < self.rows and\n",
        "                0 <= location[1] < self.cols and\n",
        "                location not in self.walls)\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        move = self.actions[action]\n",
        "        next_state = [state[0] + move[0], state[1] + move[1]]\n",
        "\n",
        "        if not self.is_valid_location(next_state):\n",
        "            next_state = state\n",
        "\n",
        "        # Check if next state is within safety distance of any negative reward\n",
        "        if self.success_probability < 0.5:\n",
        "            for dr in range(-self.safety_distance, self.safety_distance + 1):\n",
        "                for dc in range(-self.safety_distance, self.safety_distance + 1):\n",
        "                    neighbor_row = next_state[0] + dr\n",
        "                    neighbor_col = next_state[1] + dc\n",
        "                    if (0 <= neighbor_row < self.rows and\n",
        "                        0 <= neighbor_col < self.cols and\n",
        "                        self.rewards[neighbor_row][neighbor_col] < 0):\n",
        "                        # Avoid moving towards negative reward if success_probability is low\n",
        "                        if random.uniform(0, 1) > self.success_probability:\n",
        "                            return state  # Stay in the current state\n",
        "\n",
        "        return next_state\n",
        "\n",
        "    def value_iteration(self, theta=0.0001):\n",
        "        priority_queue = []\n",
        "        for row in range(self.rows):\n",
        "            for col in range(self.cols):\n",
        "                if [row, col] not in self.walls:\n",
        "                    heapq.heappush(priority_queue, (0, (row, col)))\n",
        "\n",
        "        while priority_queue:\n",
        "            delta, (row, col) = heapq.heappop(priority_queue)\n",
        "            old_value = self.value_table[row, col]\n",
        "            new_value = float('-inf')\n",
        "            best_action = None\n",
        "\n",
        "            for action in range(4):\n",
        "                action_value = 0\n",
        "                main_prob = self.success_probability\n",
        "                side_prob = (1 - main_prob) / 2\n",
        "\n",
        "                main_action = action\n",
        "                left_action = (action + 1) % 4\n",
        "                right_action = (action - 1) % 4\n",
        "\n",
        "                next_state_main = self.get_next_state([row, col], main_action)\n",
        "                next_state_left = self.get_next_state([row, col], left_action)\n",
        "                next_state_right = self.get_next_state([row, col], right_action)\n",
        "\n",
        "                reward = self.rewards[row][col] + self.costs[row][col]\n",
        "\n",
        "                main_value = reward + self.gamma * self.value_table[next_state_main[0], next_state_main[1]]\n",
        "                left_value = reward + self.gamma * self.value_table[next_state_left[0], next_state_left[1]]\n",
        "                right_value = reward + self.gamma * self.value_table[next_state_right[0], next_state_right[1]]\n",
        "\n",
        "                action_value += main_prob * main_value\n",
        "                action_value += side_prob * left_value\n",
        "                action_value += side_prob * right_value\n",
        "\n",
        "                if action_value > new_value:\n",
        "                    new_value = action_value\n",
        "                    best_action = action\n",
        "\n",
        "            self.value_table[row, col] = new_value\n",
        "            self.policy[row, col] = best_action\n",
        "            diff = abs(old_value - new_value)\n",
        "\n",
        "            if diff > theta:\n",
        "                for action in range(4):\n",
        "                    next_state = self.get_next_state([row, col], action)\n",
        "                    if self.is_valid_location(next_state):\n",
        "                        heapq.heappush(priority_queue, (-diff, tuple(next_state)))\n",
        "\n",
        "    def evaluate_policy(self, episodes=10000, max_steps=100, epsilon=0.1):\n",
        "        total_return = 0\n",
        "        for episode in range(episodes):\n",
        "            state = [0, 0]\n",
        "            episode_return = 0\n",
        "            step = 0\n",
        "            while step < max_steps:\n",
        "                if random.uniform(0, 1) < epsilon:\n",
        "                    action = random.choice(range(4))\n",
        "                else:\n",
        "                    action = self.policy[state[0], state[1]]\n",
        "                next_state = self.get_next_state(state, action)\n",
        "                reward = self.rewards[state[0]][state[1]] + self.costs[state[0]][state[1]]\n",
        "                episode_return += reward\n",
        "                if self.rewards[state[0]][state[1]] != 0 and self.costs[state[0]][state[1]] <= 0:\n",
        "                    break  # Terminate the episode at a terminal state with non-positive cost\n",
        "                state = next_state\n",
        "                step += 1\n",
        "            total_return += episode_return\n",
        "        return total_return / episodes\n",
        "\n",
        "    def print_policy(self):\n",
        "        direction_mapping = ['→', '←', '↓', '↑']\n",
        "        policy = [[None for _ in range(self.cols)] for _ in range(self.rows)]\n",
        "        for row in range(self.rows):\n",
        "            for col in range(self.cols):\n",
        "                if [row, col] in self.walls:\n",
        "                    policy[row][col] = 'W'  # Wall\n",
        "                elif self.rewards[row][col] > 0:\n",
        "                    policy[row][col] = 'P'  # Positive reward\n",
        "                elif self.rewards[row][col] < 0:\n",
        "                    policy[row][col] = 'N'  # Negative reward\n",
        "                else:\n",
        "                    best_action = self.policy[row, col]\n",
        "                    policy[row][col] = direction_mapping[best_action]\n",
        "        for row in policy:\n",
        "            print(\" \".join(row))\n",
        "\n",
        "    def print_utilities(self):\n",
        "        utilities_table = []\n",
        "        for row in range(self.rows):\n",
        "            row_values = []\n",
        "            for col in range(self.cols):\n",
        "                if [row, col] in self.walls:\n",
        "                    row_values.append(\"W\")  # Wall\n",
        "                elif self.rewards[row][col] > 0:\n",
        "                    row_values.append(\"{:.2f}\".format(self.rewards[row][col]))  # Positive reward value\n",
        "                elif self.rewards[row][col] < 0:\n",
        "                    row_values.append(\"{:.2f}\".format(self.rewards[row][col]))  # Negative reward value\n",
        "                else:\n",
        "                    row_values.append(\"{:.2f}\".format(self.value_table[row][col]))  # Utility value\n",
        "            utilities_table.append(row_values)\n",
        "\n",
        "        print(\"\\nUtilities:\")\n",
        "        print(tabulate(utilities_table, tablefmt=\"fancy_grid\", numalign=\"center\", stralign=\"center\"))\n",
        "\n",
        "def run_test_case(w, h, L, p, r):\n",
        "    # Initialize the rewards, costs, and walls\n",
        "    rewards = [[0 for _ in range(w)] for _ in range(h)]\n",
        "    costs = [[r for _ in range(w)] for _ in range(h)]\n",
        "    walls = []\n",
        "\n",
        "    for x, y, value in L:\n",
        "        if value == 0:\n",
        "            walls.append([h - y - 1, x])  # Convert (x, y) to (row, col) with (0,0) at lower-left\n",
        "        else:\n",
        "            rewards[h - y - 1][x] = value\n",
        "\n",
        "    # Define success probabilities\n",
        "    success_probability = p\n",
        "\n",
        "    # Create the GridGameMBRL instance\n",
        "    game_mbrl = GridGameMBRL(h, w, rewards, success_probability, costs, walls)\n",
        "\n",
        "    # Perform value iteration to find the best policy\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    game_mbrl.value_iteration()\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Evaluate the policy\n",
        "    policy_score = game_mbrl.evaluate_policy(episodes=1000, max_steps=100, epsilon=0.1)\n",
        "    print(\"Policy Score (Value Iteration):\", policy_score)\n",
        "    print(f\"Value Iteration Time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "    # Print the policy and utilities for visual comparison\n",
        "    print(\"\\nPolicy:\")\n",
        "    game_mbrl.print_policy()\n",
        "\n",
        "    # Print utilities\n",
        "    game_mbrl.print_utilities()\n",
        "\n",
        "def main():\n",
        "    test_cases = [\n",
        "        (4, 3, [(1,1,0),(3,2,1),(3,1,-1)], 0.8, -0.04),\n",
        "        (4, 3, [(1,1,0),(3,2,1),(3,1,-1)], 0.8, 0.04),\n",
        "        (4, 3, [(1,1,0),(3,2,1),(3,1,-1)], 0.8, -1),\n",
        "        (12, 4, [(1,0,-100),(2,0,-100),(3,0,-100),(4,0,-100),(5,0,-100),(6,0,-100),(7,0,-100),(8,0,-100),(9,0,-100),(10,0,-100),(11,0,1)], 1, -1),\n",
        "        (12, 6, [(1,0,-100),(2,0,-100),(3,0,-100),(4,0,-100),(5,0,-100),(6,0,-100),(7,0,-100),(8,0,-100),(9,0,-100),(10,0,-100),(11,0,1)], 0.9, -1),\n",
        "        (5, 5, [(4,0,-10),(0,4,-10),(1,1,1),(3,3,2)], 0.9, -0.5),\n",
        "        (5, 5, [(2,2,-2),(4,4,-1),(1,1,1),(3,3,2)], 0.9, -0.25),\n",
        "        (7, 7, [(1,1,-4),(1,5,-6),(5,1,1),(5,5,4)], 0.8, -0.5),\n",
        "        (7, 7, [(1,1,-4),(1,5,-6),(5,1,1),(5,5,4)], 0.8, -0.5),\n",
        "        (7, 7, [(3,1,0),(3,5,0),(1,1,-4),(1,5,-6),(5,1,1),(5,5,4)], 0.8, -0.25)\n",
        "    ]\n",
        "\n",
        "    for i, (w, h, L, p, r) in enumerate(test_cases):\n",
        "        print(f\"\\nRunning Test Case {i + 1}\")\n",
        "        run_test_case(w, h, L, p, r)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tqBcRtK-2R4P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}